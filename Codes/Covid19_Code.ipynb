{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3e0bd6-5f34-4653-9f9a-0ca8148e7b83",
   "metadata": {},
   "source": [
    "**Connecting to AWS Athena and querying data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0146bf9f-b564-4ad7-bbf4-b04e057afb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO # python3; python2: BytesID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d46e3e-d02a-4ab5-95bb-3cc62c983dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Athena\n",
    "\n",
    "AWS_ACCESS_KEY = \"AKIAW5PE3DSG6C3EEDCJ\"\n",
    "AWS_SECRET_KEY = \"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\"\n",
    "AWS_REGION = \"eu-north-1\"\n",
    "SCHEMA_NAME = \"covid_19\"\n",
    "S3_STAGING_DIR = \"s3://umid-athena-output/output/\"\n",
    "S3_BUCKET_NAME = \"umid-athena-output\"\n",
    "S3_OUTPUT_DIRECTORY = \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfea62f0-e943-423d-b972-f7e2f01c6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Athena and querying data\n",
    "\n",
    "athena_client = boto3.client(\n",
    "    \"athena\",\n",
    "    aws_access_key_id = AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key = AWS_SECRET_KEY,\n",
    "    region_name = AWS_REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684ced0f-bbb0-4271-8db9-28990183f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function downloads query results from AWS Athena and returns a pandas DataFrame\n",
    "# It also downloads the results from an S3 bucket and stores them in a temporary file\n",
    "\n",
    "Dict = {}\n",
    "def download_and_load_query_results(\n",
    "    client: boto3.client, query_response: Dict\n",
    ") -> pd.DataFrame:\n",
    "    while True:\n",
    "        try:\n",
    "            #This fucntion only loads the first 1000 rows\n",
    "            client.get_query_results(\n",
    "            QueryExecutionId=query_response[\"QueryExecutionId\"]\n",
    "            )\n",
    "            break\n",
    "        except Exception as err:\n",
    "            if \"not yet finished\" in str(err):\n",
    "                time.sleep(0.001)\n",
    "            else:\n",
    "                raise err\n",
    "    temp_file_location: str = \"athena_query_results.csv\"\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id = AWS_ACCESS_KEY,\n",
    "        aws_secret_access_key = AWS_SECRET_KEY,\n",
    "        region_name = AWS_REGION,\n",
    "    )\n",
    "    s3_client.download_file(\n",
    "        S3_BUCKET_NAME,\n",
    "        f\"{S3_OUTPUT_DIRECTORY}/{query_response['QueryExecutionId']}.csv\",\n",
    "        temp_file_location\n",
    "    )\n",
    "    return pd.read_csv(temp_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0cd38f-e959-404c-a375-3023c0db6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code executes a query on an AWS Athena database and downloads the query results into a pandas DataFrame.\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM enigma_jhud\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "enigma_jhud = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "# This code executes a query on an AWS Athena database and downloads the query results into a pandas DataFrame.\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM nytimes_data_in_usa_us_county\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "nytimes_data_in_usa_us_county = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "# This code executes a query on an AWS Athena database and downloads the query results into a pandas DataFrame.\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM nytimes_data_in_usa_us_states\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "nytimes_data_in_usa_us_states = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "# This code executes a query on an AWS Athena database and downloads the query results into a pandas DataFrame.\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM rearc_covid_19_testing_data_states_daily\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "rearc_covid_19_testing_data_states_daily = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "# This code executes a query on an AWS Athena database and downloads the query results into a pandas DataFrame.\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM rearc_covid_19_testing_data_us_daily\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "rearc_covid_19_testing_data_us_daily = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "# This code executes a query on an AWS Athena database and downloads the query results into a pandas DataFrame.\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM rearc_covid_19_testing_data_us_total_latest\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "rearc_covid_19_testing_data_us_total_latest = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "# This code executes a query on an AWS Athena database and downloads the query results into a pandas DataFrame.\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM rearc_usa_hospital_beds\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "rearc_usa_hospital_beds = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "# This code executes a query on an AWS Athena database and downloads the query results into a pandas DataFrame.\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM static_datasets_countrycodeqs\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "static_datasets_countrycodeqs = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "# This code executes a query on an AWS Athena database and downloads the query results into a pandas DataFrame.\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM static_datasets_county_population\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "static_datasets_county_population = download_and_load_query_results(athena_client, response)\n",
    "\n",
    "# This code executes a query on an AWS Athena database and downloads the query results into a pandas DataFrame.\n",
    "\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=\"SELECT * FROM static_datasets_states_abv\",\n",
    "    QueryExecutionContext={\"Database\": SCHEMA_NAME},\n",
    "    ResultConfiguration={\n",
    "        \"OutputLocation\": S3_STAGING_DIR,\n",
    "        \"EncryptionConfiguration\": {\"EncryptionOption\": \"SSE_S3\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "static_datasets_states_abv = download_and_load_query_results(athena_client, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "830cc963-5597-49b6-bf77-2865333b3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the first row of a Pandas DataFrame to the variable `new_header`\n",
    "\n",
    "new_header = static_datasets_states_abv.iloc[0] \n",
    "\n",
    "# Remove the first row of the DataFrame using indexing, and assign the resulting DataFrame back\n",
    "\n",
    "static_datasets_states_abv = static_datasets_states_abv[1:]\n",
    "\n",
    "# Set the column names of the DataFrame to the values in the `new_header` variable.\n",
    "\n",
    "static_datasets_states_abv.columns = new_header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb387bc-836d-4b32-8aaa-9e99c0a816d9",
   "metadata": {},
   "source": [
    "**ETL job in Python: converting relational data model into dimensional data model by building a STAR schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df23306-a276-4874-9b7c-9a50915e5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from two different data sources based on a common key\n",
    "\n",
    "factCovid_1 = enigma_jhud[['fips', 'province_state', 'country_region', 'confirmed', 'deaths', 'recovered', 'active']]\n",
    "factCovid_2 = rearc_covid_19_testing_data_states_daily[['fips', 'date', 'positive', 'negative', 'hospitalizedcurrently', 'hospitalized', 'hospitalizeddischarged']]\n",
    "factCovid = pd.merge(factCovid_1, factCovid_2, on ='fips', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2d592ce-f690-41d1-8eb2-232648c0cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from two different data sources based on a common key\n",
    "\n",
    "dimRegion_1 = enigma_jhud[['fips', 'province_state', 'country_region', 'latitude', 'longitude']]\n",
    "dimRegion_2 = nytimes_data_in_usa_us_county[['fips', 'county', 'state']]\n",
    "demRegion = pd.merge(dimRegion_1, dimRegion_2, on='fips', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa5f85c7-17d0-4fa9-ad03-e7a205e477b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 1% of the rows and create a new DataFrame\n",
    "\n",
    "demRegion = demRegion.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11df890c-68d5-420b-8058-81c966caa701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame dimHospital from the columns of another DataFrame\n",
    "\n",
    "dimHospital = rearc_usa_hospital_beds[['fips', 'state_name', 'latitude', 'longtitude', 'hq_address', 'hospital_name', 'hospital_type', 'hq_city', 'hq_state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "630a7d9a-151c-488e-a4a9-6ce823c6145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame dimDate from the 'fips' and 'date' columns of another DataFrame\n",
    "\n",
    "dimDate = rearc_covid_19_testing_data_states_daily[['fips', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d26f792e-4c72-4ff2-aeb2-108946df2115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-58b6e2fd6426>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dimDate['date'] = pd.to_datetime(dimDate['date'], format ='%Y%m%d')\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'date' column to a pandas datetime format using the pd.to_datetime() function\n",
    "\n",
    "dimDate['date'] = pd.to_datetime(dimDate['date'], format ='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f43bde-5ae9-4354-966a-214b27b4ae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-ee1bb838122c>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dimDate['year'] = dimDate['date'].dt.year\n",
      "<ipython-input-14-ee1bb838122c>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dimDate['month'] = dimDate['date'].dt.month\n",
      "<ipython-input-14-ee1bb838122c>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dimDate['day_of_week'] = dimDate['date'].dt.dayofweek\n"
     ]
    }
   ],
   "source": [
    "# Add three new columns to an existing DataFrame dimDate, based on the 'date' column in dimDate.\n",
    "\n",
    "dimDate['year'] = dimDate['date'].dt.year\n",
    "dimDate['month'] = dimDate['date'].dt.month\n",
    "dimDate['day_of_week'] = dimDate['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5ccfe79-0a1b-455a-bacc-046dc6541284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-03-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-03-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2021-03-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>2021-03-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-03-07</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20775</th>\n",
       "      <td>53</td>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20776</th>\n",
       "      <td>53</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20777</th>\n",
       "      <td>53</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20778</th>\n",
       "      <td>53</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20779</th>\n",
       "      <td>53</td>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20780 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fips       date  year  month  day_of_week\n",
       "0         2 2021-03-07  2021      3            6\n",
       "1         1 2021-03-07  2021      3            6\n",
       "2         5 2021-03-07  2021      3            6\n",
       "3        60 2021-03-07  2021      3            6\n",
       "4         4 2021-03-07  2021      3            6\n",
       "...     ...        ...   ...    ...          ...\n",
       "20775    53 2020-01-17  2020      1            4\n",
       "20776    53 2020-01-16  2020      1            3\n",
       "20777    53 2020-01-15  2020      1            2\n",
       "20778    53 2020-01-14  2020      1            1\n",
       "20779    53 2020-01-13  2020      1            0\n",
       "\n",
       "[20780 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773c174-dc8c-4c89-8ce5-9906e503dbd2",
   "metadata": {},
   "source": [
    "**Create and upload data to Amazon S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f78be3e-b8ba-43a0-90d6-5deab8b572aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the name of the S3 bucket to upload the CSV-formatted string representation\n",
    "\n",
    "bucket = 'umid-covid-project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ff648a5-099b-42e4-a74c-5a06190db4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'EHMBVZ6ZMT8FE32E',\n",
       "  'HostId': 'dBPQQwYNW7u6RaXbMdTu6yrXjNVsh1tbAnE+0kUUw7nKZSvqzbAgU7QnjlfZR4dirbhfISuS/Dk=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'dBPQQwYNW7u6RaXbMdTu6yrXjNVsh1tbAnE+0kUUw7nKZSvqzbAgU7QnjlfZR4dirbhfISuS/Dk=',\n",
       "   'x-amz-request-id': 'EHMBVZ6ZMT8FE32E',\n",
       "   'date': 'Fri, 12 May 2023 09:27:12 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"1223b14e2a14b6fd16f431061593753f\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"1223b14e2a14b6fd16f431061593753f\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CSV-formatted string representation of a DataFrame factCovid using the to_csv method of pandas.\n",
    "# The resulting CSV string is then stored in a 'StringIO' object called 'csv_buffer'\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "factCovid.to_csv(csv_buffer)\n",
    "\n",
    "# Use the boto3 library to upload a CSV-formatted string representation of a DataFrame to an Amazon S3 bucket.\n",
    "\n",
    "s3_resource = boto3.resource('s3', \n",
    "    aws_access_key_id = AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key = AWS_SECRET_KEY,\n",
    "    region_name = AWS_REGION\n",
    ")\n",
    "\n",
    "s3_resource.Object(bucket, 'output/factCovid.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "# Create a CSV-formatted string representation of a DataFrame factCovid using the to_csv method of pandas.\n",
    "# The resulting CSV string is then stored in a 'StringIO' object called 'csv_buffer'\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "demRegion.to_csv(csv_buffer)\n",
    "\n",
    "# Use the boto3 library to upload a CSV-formatted string representation of a DataFrame to an Amazon S3 bucket.\n",
    "\n",
    "s3_resource = boto3.resource('s3', \n",
    "    aws_access_key_id = AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key = AWS_SECRET_KEY,\n",
    "    region_name = AWS_REGION\n",
    ")\n",
    "\n",
    "s3_resource.Object(bucket, 'output/demRegion.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "# Create a CSV-formatted string representation of a DataFrame factCovid using the to_csv method of pandas.\n",
    "# The resulting CSV string is then stored in a 'StringIO' object called 'csv_buffer'\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "dimHospital.to_csv(csv_buffer)\n",
    "\n",
    "# Use the boto3 library to upload a CSV-formatted string representation of a DataFrame to an Amazon S3 bucket.\n",
    "\n",
    "s3_resource = boto3.resource('s3', \n",
    "    aws_access_key_id = AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key = AWS_SECRET_KEY,\n",
    "    region_name = AWS_REGION\n",
    ")\n",
    "\n",
    "s3_resource.Object(bucket, 'output/dimHospital.csv').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "# Create a CSV-formatted string representation of a DataFrame factCovid using the to_csv method of pandas.\n",
    "# The resulting CSV string is then stored in a 'StringIO' object called 'csv_buffer'\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "dimDate.to_csv(csv_buffer)\n",
    "\n",
    "# Use the boto3 library to upload a CSV-formatted string representation of a DataFrame to an Amazon S3 bucket.\n",
    "\n",
    "s3_resource = boto3.resource('s3', \n",
    "    aws_access_key_id = AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key = AWS_SECRET_KEY,\n",
    "    region_name = AWS_REGION\n",
    ")\n",
    "\n",
    "s3_resource.Object(bucket, 'output/dimDate.csv').put(Body=csv_buffer.getvalue())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890efc1-c03c-4805-b97b-fd36228e0f6a",
   "metadata": {},
   "source": [
    "**AWS Glue Transformation: copying data to Amazon Redshift**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c70fd428-9ba3-4a41-90b2-15e51df41e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"dimDate\" (\n",
      "\"index\" INTEGER,\n",
      "  \"fips\" INTEGER,\n",
      "  \"date\" TIMESTAMP,\n",
      "  \"year\" INTEGER,\n",
      "  \"month\" INTEGER,\n",
      "  \"day_of_week\" INTEGER\n",
      ")\n",
      "CREATE TABLE \"dimDate\" (\n",
      "\"index\" INTEGER,\n",
      "  \"fips\" REAL,\n",
      "  \"province_state\" TEXT,\n",
      "  \"country_region\" TEXT,\n",
      "  \"confirmed\" REAL,\n",
      "  \"deaths\" REAL,\n",
      "  \"recovered\" REAL,\n",
      "  \"active\" REAL,\n",
      "  \"date\" INTEGER,\n",
      "  \"positive\" REAL,\n",
      "  \"negative\" REAL,\n",
      "  \"hospitalizedcurrently\" REAL,\n",
      "  \"hospitalized\" REAL,\n",
      "  \"hospitalizeddischarged\" REAL\n",
      ")\n",
      "CREATE TABLE \"dimDate\" (\n",
      "\"index\" INTEGER,\n",
      "  \"fips\" REAL,\n",
      "  \"province_state\" TEXT,\n",
      "  \"country_region\" TEXT,\n",
      "  \"latitude\" REAL,\n",
      "  \"longitude\" REAL,\n",
      "  \"county\" TEXT,\n",
      "  \"state\" TEXT\n",
      ")\n",
      "CREATE TABLE \"dimDate\" (\n",
      "\"index\" INTEGER,\n",
      "  \"fips\" REAL,\n",
      "  \"state_name\" TEXT,\n",
      "  \"latitude\" REAL,\n",
      "  \"longtitude\" REAL,\n",
      "  \"hq_address\" TEXT,\n",
      "  \"hospital_name\" TEXT,\n",
      "  \"hospital_type\" TEXT,\n",
      "  \"hq_city\" TEXT,\n",
      "  \"hq_state\" TEXT\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Generates a SQL CREATE TABLE (schema) statement for the dimDate\n",
    "\n",
    "dimDatesql = pd.io.sql.get_schema(dimDate.reset_index(), 'dimDate')\n",
    "print(''.join(dimDatesql))\n",
    "\n",
    "# Generates a SQL CREATE TABLE (schema) statement for the factCovid \n",
    "\n",
    "factCovidsql = pd.io.sql.get_schema(factCovid.reset_index(), 'dimDate')\n",
    "print(''.join(factCovidsql))\n",
    "\n",
    "# Generates a SQL CREATE TABLE (schema) statement for the demRegion \n",
    "\n",
    "demRegionsql = pd.io.sql.get_schema(demRegion.reset_index(), 'dimDate')\n",
    "print(''.join(demRegionsql))\n",
    "\n",
    "# Generates a SQL CREATE TABLE (schema) statement for the dimHospital\n",
    "\n",
    "dimHospitalsql = pd.io.sql.get_schema(dimHospital.reset_index(), 'dimDate')\n",
    "print(''.join(dimHospitalsql))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33ea75-3e3b-4962-938a-9b347f45c878",
   "metadata": {},
   "source": [
    "**Use the code below as a script for AWS Glue job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "109fca41-5b48-42c5-9c03-acb6f167522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x7f9da954d5b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The redshift_connector module is a Python library that enables Python applications \n",
    "# to connect to Amazon Redshift clusters.\n",
    "\n",
    "import redshift_connector\n",
    "\n",
    "# Connecting to Redshift cluster using AWS credentials\n",
    "\n",
    "conn = redshift_connector.connect(\n",
    "    host='redshift-cluster-2.c1jvzte2gupm.eu-north-1.redshift.amazonaws.com',\n",
    "    port = 5439,\n",
    "    database='dev',\n",
    "    user=\"awsuser\",\n",
    "    password='Passw0rd123'\n",
    ")\n",
    "\n",
    "# Enables automatic commit mode for a connection object, which automatically commits SQL statements\n",
    "\n",
    "conn.autocommit = True \n",
    "\n",
    "cursor = redshift_connector.Cursor = conn.cursor()\n",
    "\n",
    "# Executing an SQL query to create a new table named \"dimDate\" in the database\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE \"dimDate\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" INTEGER,\n",
    "  \"date\" TIMESTAMP,\n",
    "  \"year\" INTEGER,\n",
    "  \"month\" INTEGER,\n",
    "  \"day_of_week\" INTEGER\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Executing an SQL query to create a new table named \"factCovid\" in the database\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE \"factCovid\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" REAL,\n",
    "  \"province_state\" TEXT,\n",
    "  \"country_region\" TEXT,\n",
    "  \"confirmed\" REAL,\n",
    "  \"deaths\" REAL,\n",
    "  \"recovered\" REAL,\n",
    "  \"active\" REAL,\n",
    "  \"date\" INTEGER,\n",
    "  \"positive\" REAL,\n",
    "  \"negative\" REAL,\n",
    "  \"hospitalizedcurrently\" REAL,\n",
    "  \"hospitalized\" REAL,\n",
    "  \"hospitalizeddischarged\" REAL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Executing an SQL query to create a new table named \"demRegion\" in the database\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE \"demRegion\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" REAL,\n",
    "  \"province_state\" TEXT,\n",
    "  \"country_region\" TEXT,\n",
    "  \"latitude\" REAL,\n",
    "  \"longitude\" REAL,\n",
    "  \"county\" TEXT,\n",
    "  \"state\" TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Executing an SQL query to create a new table named \"dimHospital\" in the database\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE \"dimHospital\" (\n",
    "\"index\" INTEGER,\n",
    "  \"fips\" REAL,\n",
    "  \"state_name\" TEXT,\n",
    "  \"latitude\" REAL,\n",
    "  \"longtitude\" REAL,\n",
    "  \"hq_address\" TEXT,\n",
    "  \"hospital_name\" TEXT,\n",
    "  \"hospital_type\" TEXT,\n",
    "  \"hq_city\" TEXT,\n",
    "  \"hq_state\" TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Executing SQL command to load data from a file in S3 into a Redshift table\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "copy dimDate from 's3://umid-covid-project/output/dimDate.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::475610684557:role/redshift_s3_access'\n",
    "delimiter ','\n",
    "region 'eu-north-1'\n",
    "IGNOREHEADER 1\n",
    "\"\"\")\n",
    "\n",
    "# Executing SQL command to load data from a file in S3 into a Redshift table\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "copy factCovid from 's3://umid-covid-project/output/factCovid.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::475610684557:role/redshift_s3_access'\n",
    "delimiter ','\n",
    "region 'eu-north-1'\n",
    "IGNOREHEADER 1\n",
    "\"\"\")\n",
    "\n",
    "# Executing SQL command to load data from a file in S3 into a Redshift table\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "copy demRegion from 's3://umid-covid-project/output/demRegion.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::475610684557:role/redshift_s3_access'\n",
    "delimiter ','\n",
    "region 'eu-north-1'\n",
    "IGNOREHEADER 1\n",
    "\"\"\")\n",
    "\n",
    "# Executing SQL command to load data from a file in S3 into a Redshift table\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "copy dimHospital from 's3://umid-covid-project/output/dimHospital.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::475610684557:role/redshift_s3_access'\n",
    "delimiter ','\n",
    "region 'eu-north-1'\n",
    "IGNOREHEADER 1\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
